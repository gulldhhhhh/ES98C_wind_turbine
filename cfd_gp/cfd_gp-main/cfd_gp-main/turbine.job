#!/bin/bash
#
# SLURM job submission script generated by MyCluster
#
#SBATCH --nodelist dedicated123.csc.warwick.ac.uk
# Job name
#SBATCH -J casename
# Send status information to this email address.
##SBATCH --mail-user=
# Send me an e-mail when the job has finished.
##SBATCH --mail-type=ALL
# Redirect output stream to this file.
#SBATCH --output casename.out.%j
# Which project should be charged
##SBATCH -A turbine
# Partition name
#SBATCH -p mnf
# Number of nodes
#SBATCH --nodes 1
# Number of tasks
# #SBATCH --ntasks 1
# Exclusive node use
#Â #SBATCH --exclusive
# Do not requeue job on node failure
#SBATCH --no-requeue
# High performance cpu governor
# #SBATCH --cpu-freq=Performance
# How much wallclock time will be required?
#SBATCH --time=12:00:00
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=28

export MYCLUSTER_QUEUE=mnf
export MYCLUSTER_JOB_NAME=windir
export NUM_TASKS=28
export TASKS_PER_NODE=14
export THREADS_PER_TASK=-1

#EDIT as needed
export ZCFDDIR=/storage/eng/phslay/zCFD-icc-sse-impi-2019.11.95-Linux-64bit

CPUS_PER_NODE=`echo ${SLURM_JOB_CPUS_PER_NODE}  | cut -d \( -f 1`
echo cpu per node $CPUS_PER_NODE
export THREADS_PER_TASK=`expr ${CPUS_PER_NODE} / ${TASKS_PER_NODE}`


export NUM_NODES=1
export JOBID=$SLURM_JOB_ID

# OpenMP configuration
export OMP_NUM_THREADS=$THREADS_PER_TASK
export OMP_PROC_BIND=true
export OMP_PLACES=sockets

# OpenMPI
export OMPI_CMD="mpiexec -n $NUM_TASKS -npernode $TASKS_PER_NODE -bysocket -bind-to-socket"
# OpenMPI 1.10
export OMPI_NEW_CMD="mpiexec -n $NUM_TASKS --map-by ppr:1:numa --bind-to numa"

# MVAPICH2
export MV2_CPU_BINDING_LEVEL=SOCKET
export MV2_CPU_BINDING_POLICY=scatter
export MVAPICH_CMD="mpiexec -n $NUM_TASKS -ppn $TASKS_PER_NODE -bind-to-socket"

# Intel MPI
# The following variables define a sensible pinning strategy for Intel MPI tasks -
# this should be suitable for both pure MPI and hybrid MPI/OpenMP jobs:
export I_MPI_PIN_DOMAIN=omp:compact # Domains are $OMP_NUM_THREADS cores in size
export I_MPI_PIN_ORDER=scatter # Adjacent domains have minimal sharing of caches/sockets
#export I_MPI_FABRICS=shm:ofa
#export I_MPI_FABRICS=shm:tmi
#export TMI_CONFIG=<path_to_impi>/intel64/etc/tmi.conf
export IMPI_CMD="mpiexec -n $NUM_TASKS -ppn $TASKS_PER_NODE"

# Summarise environment
echo -e "JobID: $JOBID\n======"
echo "Time: `date`"
echo "Running on master node: `hostname`"
echo "Current directory: `pwd`"

if [ "$SLURM_JOB_NODELIST" ]; then
        #! Create a machine file:
        echo $SLURM_JOB_NODELIST | uniq > machine.file.$JOBID
        echo -e "\nNodes allocated:\n================"
        echo `cat machine.file.$JOBID | sed -e 's/\..*$//g'`
fi

echo -e "\nnumtasks=14, numnodes=1, tasks_per_node=14 (OMP_NUM_THREADS=$OMP_NUM_THREADS)"

echo -e "\nExecuting command:\n==================\n$ZCFDDIR/share/MyCluster/mycluster-zcfd.bsh\n"

# Run user script
cd $ZCFDDIR/bin/
source activate
cd -
$ZCFD_HOME/python make_turbine_zones.py
$ZCFD_HOME/python make_new_zones.py
$ZCFD_HOME/run_zcfd --ntask 14 -p turbine.h5 -c casename
exitcode=$?

# Report on completion
echo -e "\nJob Complete:\n==================\n"



echo -e "Complete with exit code $exitcode========\n"
exit $exitcode
